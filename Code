# 1.Input Layer--CN/DenseLayer/Convoluted Layer
#     i. Relu Layer
#     ii.Pooling Layer  
#       iii.Upsampling 
     
#         i.batchNormalization
#         ii.Droupout Layer
#         
    
# 2.HiddenLayer --NN
# 3.Output Layer--FCNN  



1.Tensorflow
2.Pytorch
3.MaxNet

1.Numpy
2.Pandas
3.Pillow
4.OpenCV
5. Scikit Learn

i.plotly
ii.matlplotlib
iii.Seaborn
iv.D3.js
v.NLTK

Importing Library

# This section for model
import tensorflow as tf
#This section for algebric analysis
import numpy as np
# This section for visualisation
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline


help(warnings.filterwarnings)


# value = dict({"product_price1":10,"product_price": 100})


# 1.Random: import random as rnd
# 2.Numpy: import numpy.random as npr
# 3.Tensorflow: import tensorflow.random 


class NeuralNetwork:
    def __init__(self,layers):
        self.layers=layers
        self.L=len(layers)
        
        self.number_feature=layers[0]
        self.number_class= layers[-1]
        
        self.w ={}
        self.b ={}
        
        self.dw = {}
        self.db = {}
        
        self.setup()
    
    def setup(self):
        for i in range(1,self.L):
            self.w[i] = tf.Variable(tf.random.normal(shape= (self.layers[i],self.layers[i-1])))
            self.b[i] = tf.Variable(tf.random.narmal(shape= (self.layers[i],1)))
        
        
# 1.dtype="float32"
# 2.dtype=np.float32
# 3.dtype=tf.float32
# 4.dtype=torch.float32

# Forward propagation
class NeuralNetwork(NeuralNetwork):
    def forwardPass (self,A):
        A = tf.convert_to_tensor(A, dtype=tf.float32)
        for i in range(1,self.L):
            Z = tf.matmul(A, tf.transpose(self.w[i])) + tf.transpose(self.b[i])
            if i!= self.L-1:
                A = tf.nn.relu(Z)
            else:
                A=Z
        return A       
        
        
   # We will use this section for computing loss and Upgrading the previous parameters
class NeuralNetwork(NeuralNetwork):
    def compute_loss(self, A,Y):
        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(Y,A))
    
    def upgrade_parameters(self, lr):
        for j in range(1,self.L):
            self.w[j].assing_sub(lr*self.dw[j])
            self.b[j].assing_sub(lr*self.db[j])
            
# 0 index-->cats
# 1 index-->dogs
np.argmax([0.12,0.09])         





            
        
 class NeuralNetwork(NeuralNetwork):
    def predict(self, x):
        A = self.forwardPass(x)
        return tf.argmax(tf.nn.softmax(A), axis = 1)
    def info(self):
        num_params = 0
        for i in range(1,self.L):
            num_params += self.w[i].shape[0]*self.w[i].shape[1]
            num_params += self.b[i].shape[0]
            
        print("Number of Feature: {}".format(self.number_feature))
        print("Total Number of Class is: {}". format(self.number_class))
        
        print("Hidden Layer information is:")
        for j in range(1,self.L-1):
            print("Layer: {}, Units {}".format(iself.layers[j]))
            
        print ("Total Number of parameters : {}". format(num_params)) 
        
        
        
        
 # history = model.fit
 
 
 # Training Start
class NeuralNetwork(NeuralNetwork):
    def train(self,x_train,y_train,x_test,y_test, epochs, step_per_epochs, batch_size, lr):
        history = {"val_loss":[],
                  'train_loss':[],
                  'val_acc':[]}
                  
                  
            for e in range(0, epochs):
            epochs_loss_train = 0.0
            print("Epochs {}". format(e),end = " | ")
            for i in range(step_per_epochs):
                x_batch = x_train[i*batch_size : (i+1)*batch_size]
                y_batch = y_train[i*batch_size : (i+1)*batch_size]
                batch_loss = self.training_on_batch(x_batch, y_batch, lr)
                epochs_loss_train += batch_loss
                
        
                if i %int(step_per_epochs/10) ==0:
                    print(end = " . ")
                    
            
            
            history['train_loss'].append(epochs_loss_train/step_per_epochs)   


            valA = self.forwardPass(x_test)   
            history['val_loss'].append(self.compute_loss(y_test, valA).numpy())


            valuePr = self.predict(x_test)
            history['val_acc'].append(np.mean(np.argmax(y_test, axis = 1)== valuePr.numpy()))
            print("value Accuracy:", history['val_acc'][-1])

    # Here We will get training information
    
        return history
            
            
            
            def loadDatasets():
    (x_train, y_train),(x_test,y_test)= tf.keras.datasets.mnist.load_data()
    x_train = np.reshape(x_train, (x_train.shape[0],784))/255.0
    y_train = tf.keras.utils.to_categorical(y_train)
    x_test = np.reshape(x_test,(x_test.shape[0],784))/255.0
    y_test = tf.keras.utils.to_categorical(y_test)
    
    print("X Train Datasets Shape: {}". format(x_train.shape))
    print("Y test Data {}".format(y_test))
    print ("Len of Y test Datasets:{}".format(len(y_test[0])))
   # print("Unique Value is :{}".format(np.unique(y_test[0]))) 
    print("Unique Value is :{}".format(np.argmax(y_test[7])))
    return (x_train, y_train), (x_test, y_test)


def plot_random_sample(x,y,p= None):
    indicies = np.random.choice(range(0,x.shape[0]),10)
    y=np.argmax(y, axis=1)
    
    
    if p is None:
        p = y
        
    plt.figure(figsize=(10,4)) 
    for i, index in enumerate(indicies):
        plt.subplot(2,5, i+1)
        plt.imshow(x[index].reshape((28,28)), cmap = 'binary')
        plt.xticks([])
        plt.yticks([])
        plt.gray()
        
        
        if y[index]== p[index]:
            col = 'g'
        else:
            col = 'r'
        plt.xlabel(str(p[index]), color = col)
            
            
    return plt       
    
    
    
    (x_train, y_train),(x_test,y_test)=loadDatasets()
    
    
    
    plot_random_sample(x_train,y_train).show()
    
    network = NeuralNetwork([784,256,256,10])
    
     network.info()
     batch_size =128
     epochs = 20
     step_per_epochs = int(x_train.shape[0]/batch_size)
     
     
     X_train.shape[0]//128
     learning_rate = 3e-6
     
     history = network.train(x_train,y_train,x_test,y_test, epochs, step_per_epochs, batch_size, learning_rate)
     
     
     def plot_results(history):
    plt.figure(figsize=(12,4))
    epochs = len(history['val_loss'])
    plt.subplot(1,2,1)
    plt.plot(range(epochs), history['val_loss'], label='Val Loss')
    plt.plot(range(epochs), history['train_loss'], label='Train Loss')
    plt.xticks(list(range(epochs)))
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend() 
    
    
    plt.subplot(1,2,2)
    plt.plot(range(epochs), history['val_acc'], label = 'Val Acc')
    plt.xticks(list(range(epochs)))
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    return plt
    
  plot_results(history).show()
  
  pred = network.predict(x_test)
  
  
  plot_random_sample(x_test,y_test,pred.numpy()).show()
  
  
  
  
